{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50c42f67",
   "metadata": {},
   "source": [
    "# Select the correct environment and import necessary modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953b3de9",
   "metadata": {},
   "source": [
    "Select the correct kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f4d619",
   "metadata": {},
   "source": [
    "I recommend creating a new environment. Then to select it :\n",
    "- Select Kernel\n",
    "    - Select another kernel\n",
    "        - Jupyter kernel\n",
    "            - Smart Resume an Job matcher (or any kernel name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b9796a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "999a6481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sebastien\\Desktop\\LLM and GenAI\\Smart_Resume_to_Job_Matcher\\.venv\\Scripts\\python.exe\n"
     ]
    }
   ],
   "source": [
    "print(sys.executable)\n",
    "# Should see something like : ...\\Smart_Resume_to_Job_Matcher\\.venv\\Scripts\\python.exe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3fad394",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib# To reload modified python files for agents\n",
    "import sys# To find agents python files\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc3e4c2",
   "metadata": {},
   "source": [
    "# Pull models and embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd6cb1f",
   "metadata": {},
   "source": [
    "In a first terminal window, launch the ollama app:\n",
    "~~~\n",
    "ollama serve\n",
    "~~~\n",
    "\n",
    "Then pull the model and embedding from another terminal window:\n",
    "~~~\n",
    "ollama pull llama3\n",
    "ollama pull nomic-embed-text\n",
    "ollama pull mxbai-embed-large\n",
    "~~~\n",
    "\n",
    "To eventually delete all models, use:\n",
    "~~~\n",
    "ollama list\n",
    "ollama rm <model-name>\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a0fc21",
   "metadata": {},
   "source": [
    "# Resume and Job description loader  **DONE**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09058eff",
   "metadata": {},
   "source": [
    "~~~\n",
    "project/\n",
    "├── data/\n",
    "│ ├── resumes/\n",
    "| | └── resumes.csv # already partially preprocessed\n",
    "│ └── jobs/\n",
    "| | └── job_postings.csv # filtered with only the real jobs applications\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c6fdee",
   "metadata": {},
   "source": [
    "In this part, we need to transform each resume and job description into one single text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "576c2984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the parent folder of ingestion to sys.path\n",
    "project_root = Path(\"..\").resolve()  # notebooks/ is one level down\n",
    "sys.path.append(str(project_root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bb05d25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'ingestion.preprocess' from 'C:\\\\Users\\\\Sebastien\\\\Desktop\\\\LLM and GenAI\\\\Smart_Resume_to_Job_Matcher\\\\ingestion\\\\preprocess.py'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This code is to reload the modified python files for the agent\n",
    "import ingestion.preprocess\n",
    "# Reload the file to take into account the changes\n",
    "importlib.reload(ingestion.preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f494ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ingestion.preprocess import clean_text, resumes_to_raw_text, jobs_to_raw_text\n",
    "# Load resumes and job only reads the csv file and transform each line into a json like format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "274d34de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we get the resumes and jobs as lists of formatted JSON files, \n",
    "#   with the id and formatted text for each file\n",
    "resumes = resumes_to_raw_text(\"../data/resumes/resumes.csv\")\n",
    "jobs = jobs_to_raw_text(\"../data/jobs/job_postings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0d63da0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 13389 resumes\n",
      "Loaded 17014 job descriptions\n",
      "Sample resume: {'resume_id': 483, 'text': 'Category: Aviation. Resume text: qualifications stacy sample 702 8000000 cell 0000emailcom flight ratings faa atpairline transport pilot cfirotorcrafthelicopter s70 s92 typeratings medical flight time total 6867 pilot command 5673 night 1342 pilot 1194 night vision goggle 528 helicopter multiengine 4660 offshoreoverwater 574 faa class 1 cross country 3847 external load 263 instrument ifrhood 832 ems 541 actual weather 369 mountain 2683 experience sikorsky s76 s92 test pilot jul 2007present bell 430 corporate ifr captain jan 2006jun 2007 sikorsky s76 ems captain oct 2003dec 2005 sikorsky aircraft corporation coatesville pa s76 592 production test pilotmaintenance test pilot engineering test flight civil foreign military flight instructor customer demonstrations international ifr vfr overwater experience elite aviationlos angeles ca single pilot bell 430 corporate ifr captainoffairport landing logistics airground security company contract included occasional screen actors guild motion picture work part 91 135 childrens hospital los angeleshdinet aviationvan nuys ca single pilot s76 ems captain mountainous rooftop confined areas offday schedule included as350 576 charter corporate jan 2002sep 2003 transport film work eng maintenance test flight part 91135 350 chartertour pilot sikorsky s70 bell 205 pilot education jan 1994nov 2001 manufacturer app moved courses sikorsky 707692bell 430 master aeronautical science 2014 sundance helicopters las vegas nv grand canyon az eurocopter 350 charter pilottour charter maintenance flights high tempda high canyon winds max gross weight 5000ft mountainous terrain part 91135 united states armyus middle east s70 bell 205 pilot command night vision goggle low level external load rappel fast rope parachute drop drug intervention overwater medevac pinnacle confined area desert high tempda 10000ft mountainous terrain'}\n",
      "Sample job: {'job_id': 16021, 'text': 'title: Social Media/Marketing Manager. location: US, CA, Carlsbad. description: #URL_eda2500ddcedb60957fcd7f5b164e092966f8c4e8fb89ce70a16bea1545a297d#, a growing social media company, is seeking an experienced Social Media/Marketing Manager to be the visionary of our strategy with social media. In this role you will be a founding member of Businessfriend’s core team focused on generating user traffic and executing global marketing programs for “The Ultimate Business App.\"#URL_eda2500ddcedb60957fcd7f5b164e092966f8c4e8fb89ce70a16bea1545a297d# combines the ability to discover, connect and share with fellow professionals and offers the unique ability to communicate, manage, store and notate all aspects of your working life from one website and one mobile app. It’s positioned to be the world\\'s newest social networking platform for professionals and the companies they work for.Do you have what it takes to be part of “The Next Big Thing” to hit social media? Read on…. requirements: The Social Media/Marketing Manager will use social media programs as a way to build community with Businessfriend users and attract and engage future users. The ideal candidate, given the position’s high impact, will be passionate about figuring out the best way to reach and share information across social media platforms using both paid and earned media. This position is an integral part of building the Businessfriend brand. Important: Qualified candidates will have prior experience managing Social Media for a large organization or major agency. Key Responsibilities: Develop a social media strategy designed to grow awareness and consideration of Businessfriend and increase user baseManage the execution of innovative social initiatives Deliver content and programs with high engagement rates and strong positive brand buzz Build and manage social communities Partner with Marketing team to ensure social amplification of key communications to users and publicBe the go-to-resource on internal communications execution using best practices and policies Stay on top of social media trendsTrack and measure success of social media programs Required Skills &amp; Experience: We don’t care about your degree or number of years of work experience – we want to know what you’ve accomplished in the social media arena!!!!!!Proven track record building brands via Facebook, Twitter, LinkedIn, Foursquare, Instagram, etc. Excellent communication, interpersonal and leadership skills Ability to multi-task and manage multiple priorities Experience using various social, SEM and web analytics reporting tools Strong analytics, focused on developing results-driven campaigns Self-motivated, organized, and a strategic thinker. employment_type: Full-time. required_experience: . required_education: Unspecified. industry: Internet. function: Marketing'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Loaded\", len(resumes), \"resumes\")\n",
    "print(\"Loaded\", len(jobs), \"job descriptions\")\n",
    "\n",
    "print(\"Sample resume:\", resumes[482])\n",
    "print(\"Sample job:\", jobs[15489])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1040a4",
   "metadata": {},
   "source": [
    "# Normalize text **DONE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "3bac30f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'agents.normalization_agent' from 'C:\\\\Users\\\\Sebastien\\\\Desktop\\\\LLM and GenAI\\\\Smart_Resume_to_Job_Matcher\\\\agents\\\\normalization_agent.py'>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import agents.normalization_agent\n",
    "# Reload the file to take into account the changes\n",
    "importlib.reload(agents.normalization_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e006b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.normalization_agent import normalize_resume, normalize_job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f195d1a4",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed52db14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category: Accountant. Resume text: education omba executive leadership university texas 20162018 bachelor science accounting richland college 20052008 training certifications certified management accountant cma certified financial modeling valuation analyst compliance antimoney laundering 092016 american institute banking certified public account cpa lean six sigma green belt certified trade products financial regulations 082016 american institute banking achievements speaker bringing leader within 082019 successfully presented empowering speech leadership 500 participants speaker dallas convention cpas 032019 successfully delivered seminar 3k cpas convention guests teaching experience online teacher udemy 2017 taught online accounting nonaccountant course udemy similar online teaching platforms developed effective teaching modules materials curriculum target students took feedbacks students assist improving teaching methodology materials professional memberships affiliations american society executives 2018 present technical skills quickbooks erp sap oracle hyperion languages english native fellow chartered accountant 2011 present ms office sql ibm cognos german spanish french full professional proficiency limited working proficiency limited working proficiency interests artificial intelligence chess nnovoresumecom sailing cryptocurrencies page 2 2\n",
      "title: Marketing Intern. location: US, NY, New York. description: Food52, a fast-growing, James Beard Award-winning online food community and crowd-sourced and curated recipe hub, is currently interviewing full- and part-time unpaid interns to work in a small team of editors, executives, and developers in its New York City headquarters.Reproducing and/or repackaging existing Food52 content for a number of partner sites, such as Huffington Post, Yahoo, Buzzfeed, and more in their various content management systemsResearching blogs and websites for the Provisions by Food52 Affiliate ProgramAssisting in day-to-day affiliate program support, such as screening affiliates and assisting in any affiliate inquiriesSupporting with PR &amp; Events when neededHelping with office administrative work, such as filing, mailing, and preparing for meetingsWorking with developers to document bugs and suggest improvements to the siteSupporting the marketing and executive staff. requirements: Experience with content management systems a major plus (any blogging counts!)Familiar with the Food52 editorial voice and aestheticLoves food, appreciates the importance of home cooking and cooking with the seasonsMeticulous editor, perfectionist, obsessive attention to detail, maddened by typos and broken links, delighted by finding and fixing themCheerful under pressureExcellent communication skillsA+ multi-tasker and juggler of responsibilities big and smallInterested in and engaged with social media like Twitter, Facebook, and PinterestLoves problem-solving and collaborating to drive Food52 forwardThinks big picture but pitches in on the nitty gritty of running a small company (dishes, shopping, administrative support)Comfortable with the realities of working for a startup: being on call on evenings and weekends, and working long hours. employment_type: Other. required_experience: Internship. required_education: . industry: . function: Marketing\n"
     ]
    }
   ],
   "source": [
    "# Now, we try it on the first resume and job text\n",
    "resume_text = resumes[0][\"text\"]\n",
    "job_text = jobs[0][\"text\"]\n",
    "print(resume_text)\n",
    "print(job_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "37dcf0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'skills': ['Microsoft Office', 'QuickBooks', 'ERP', 'SAP', 'Oracle', 'Hyperion', 'SQL'], 'experience': ['Accountant – Omba Executive Leadership (2016-2018): Managed financial regulations and compliance.', 'Speaker, Dallas Convention CPAs (2019): Successfully presented empowering speech to 500 participants.'], 'education': ['Bachelor of Science in Accounting, Richland College (2005-2008)', 'Executive Leadership, University of Texas (2016-2018)'], 'certifications': ['CMA', 'CFMVA', 'CPA', 'Lean Six Sigma Green Belt'], 'industries': ['Financial Regulations']}\n",
      "{'job_title': 'Marketing Intern', 'required_skills': ['Content management systems', 'Food52 editorial voice and aesthetic', 'Social media (Twitter, Facebook, Pinterest)', 'Problem-solving', 'Collaboration'], 'required_experience': 'Internship', 'required_education': 'Not specified', 'industry': 'Media'}\n"
     ]
    }
   ],
   "source": [
    "# The llama model is initialized inside the agent (so we only need to run the agent)\n",
    "normalized_resume = normalize_resume(resume_text)\n",
    "normalized_job = normalize_job(job_text)\n",
    "\n",
    "print(normalized_resume)\n",
    "print(normalized_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe84fa05",
   "metadata": {},
   "source": [
    "/!\\ INFO /!\\\n",
    " - RESUMES\n",
    "\n",
    "Preprocess gives in a text the whole resume text\n",
    "\n",
    "For normalized resumes, should give the infos :\n",
    "{'skills': [], 'experience': [], 'education': [], 'certifications': [], 'industries': []}\n",
    "\n",
    "\n",
    " - JOB DESCRIPTIONS\n",
    "\n",
    "Preprocess gives in a text : title, location, description, requirements, employment_type, required_experience, required_education, industry, function\n",
    "\n",
    "For normalized job, should give the following infos : \n",
    "{'job_title': '...', 'required_skills': [], 'required_experience': '', 'required_education': '', 'industry': ''}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc63599",
   "metadata": {},
   "source": [
    "### Complete normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2142c42",
   "metadata": {},
   "source": [
    "Now that we have an example with one resume and one job, we need to do it for all resumes and jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964a6cd9",
   "metadata": {},
   "source": [
    "In this step, I decided to put normalized resumes and jobs into two distinct lists, each containing the normalized representation of resumes and job descriptions. The objects will look like this :\n",
    "```python \n",
    "normalized_resumes = [\n",
    "    {'resume_id': 1, \n",
    "    'norm_text': {'skills': [...], 'experience': [...], 'education': [...], 'certifications': [...], 'industries': [...]}\n",
    "    },\n",
    "    ...\n",
    "]\n",
    "\n",
    "normalized_jobs = [\n",
    "    {'job_id': 1, \n",
    "    'job_description': {'job_title': '...', 'required_skills': [...], 'required_experience': '...', 'required_education': '...', 'industry': '...'}\n",
    "    },\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a57a2ee",
   "metadata": {},
   "source": [
    "However, since the quantity of resumes and job description is very high, the computation takes time. In case it crashes in the middle, I created a checkpoint file to save progress every 10 normalized elements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb45ab86",
   "metadata": {},
   "source": [
    "/!\\ \n",
    "**Léger problème** : les skills ne semblent pas normalizés. J'ai pu voir \"MS Office\" mais aussi \"Microsoft Excel\", \"Microsoft Word\" et \"Microsoft Office\", alors que j'ai explicitement demander la normalization dans le prompt pour le llm.\n",
    "\n",
    "-> Problème réglé grâce à une \"normalization forcée\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770f120d",
   "metadata": {},
   "source": [
    "#### Resumes (13389 elements) **DONE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cff458a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6fb8f4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint found\n",
      "12020 resumes\n"
     ]
    }
   ],
   "source": [
    "# We first prepare checkpoints (very long computation)\n",
    "## Checkpoint directory to save partial results\n",
    "checkpoint_dir = Path(\"checkpoints\")\n",
    "checkpoint_dir.mkdir(exist_ok=True)\n",
    "\n",
    "## File to save intermediate results\n",
    "resume_checkpoint = checkpoint_dir / \"normalized_resumes.json\"\n",
    "\n",
    "## Load existing checkpoint if available\n",
    "if resume_checkpoint.exists():\n",
    "    print(\"Checkpoint found\")\n",
    "    with open(resume_checkpoint, \"r\") as f:\n",
    "        normalized_resumes = json.load(f)\n",
    "    processed_ids = {r['resume_id'] for r in normalized_resumes}\n",
    "    print(len(processed_ids), \"resumes\")\n",
    "else:\n",
    "    normalized_resumes = []\n",
    "    processed_ids = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6975258b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalizing resumes:  90%|████████▉ | 12030/13389 [00:45<00:10, 132.53it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Attempt 1 failed for resume. Retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalizing resumes:  91%|█████████ | 12120/13389 [08:13<1:48:26,  5.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Attempt 1 failed for resume. Retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalizing resumes:  91%|█████████ | 12209/13389 [16:02<1:51:22,  5.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Attempt 1 failed for resume. Retrying...\n",
      "⚠ Attempt 2 failed for resume. Retrying...\n",
      "⚠ Attempt 3 failed for resume. Retrying...\n",
      "❌ Failed to normalize resume after 3 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalizing resumes:  91%|█████████▏| 12224/13389 [17:45<2:03:31,  6.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Attempt 1 failed for resume. Retrying...\n",
      "⚠ Attempt 2 failed for resume. Retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalizing resumes:  91%|█████████▏| 12225/13389 [18:11<3:56:29, 12.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Attempt 3 failed for resume. Retrying...\n",
      "❌ Failed to normalize resume after 3 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalizing resumes:  92%|█████████▏| 12332/13389 [28:03<1:39:43,  5.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Attempt 1 failed for resume. Retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalizing resumes:  93%|█████████▎| 12439/13389 [37:07<1:06:59,  4.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Attempt 1 failed for resume. Retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalizing resumes:  94%|█████████▍| 12629/13389 [51:30<1:07:53,  5.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Attempt 1 failed for resume. Retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalizing resumes:  95%|█████████▍| 12669/13389 [55:35<1:10:07,  5.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Attempt 1 failed for resume. Retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalizing resumes:  95%|█████████▌| 12739/13389 [1:02:12<44:32,  4.11s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Attempt 1 failed for resume. Retrying...\n",
      "⚠ Attempt 2 failed for resume. Retrying...\n",
      "⚠ Attempt 3 failed for resume. Retrying...\n",
      "❌ Failed to normalize resume after 3 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalizing resumes:  95%|█████████▌| 12752/13389 [1:03:46<56:10,  5.29s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Attempt 1 failed for resume. Retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalizing resumes:  95%|█████████▌| 12777/13389 [1:06:15<1:03:54,  6.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Attempt 1 failed for resume. Retrying...\n",
      "⚠ Attempt 2 failed for resume. Retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalizing resumes:  95%|█████████▌| 12778/13389 [1:06:40<2:02:04, 11.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Attempt 3 failed for resume. Retrying...\n",
      "❌ Failed to normalize resume after 3 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalizing resumes:  96%|█████████▌| 12857/13389 [1:14:01<52:00,  5.87s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Attempt 1 failed for resume. Retrying...\n",
      "⚠ Attempt 2 failed for resume. Retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalizing resumes:  96%|█████████▌| 12858/13389 [1:14:27<1:45:44, 11.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Attempt 3 failed for resume. Retrying...\n",
      "❌ Failed to normalize resume after 3 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalizing resumes:  96%|█████████▌| 12877/13389 [1:16:08<47:40,  5.59s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Attempt 1 failed for resume. Retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalizing resumes:  97%|█████████▋| 12975/13389 [1:24:54<33:55,  4.92s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Attempt 1 failed for resume. Retrying...\n",
      "⚠ Attempt 2 failed for resume. Retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalizing resumes:  97%|█████████▋| 12976/13389 [1:25:20<1:16:13, 11.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Attempt 3 failed for resume. Retrying...\n",
      "❌ Failed to normalize resume after 3 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalizing resumes:  98%|█████████▊| 13077/13389 [1:34:08<24:01,  4.62s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Attempt 1 failed for resume. Retrying...\n",
      "⚠ Attempt 2 failed for resume. Retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalizing resumes:  98%|█████████▊| 13078/13389 [1:34:32<54:52, 10.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Attempt 3 failed for resume. Retrying...\n",
      "❌ Failed to normalize resume after 3 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalizing resumes:  98%|█████████▊| 13116/13389 [1:37:23<19:21,  4.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Attempt 1 failed for resume. Retrying...\n",
      "⚠ Attempt 2 failed for resume. Retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalizing resumes:  98%|█████████▊| 13117/13389 [1:37:37<31:42,  7.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Attempt 3 failed for resume. Retrying...\n",
      "❌ Failed to normalize resume after 3 attempts.\n",
      "⚠ Attempt 1 failed for resume. Retrying...\n",
      "⚠ Attempt 2 failed for resume. Retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalizing resumes:  98%|█████████▊| 13118/13389 [1:38:02<56:53, 12.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Attempt 3 failed for resume. Retrying...\n",
      "❌ Failed to normalize resume after 3 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalizing resumes:  98%|█████████▊| 13136/13389 [1:39:19<18:13,  4.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Attempt 1 failed for resume. Retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalizing resumes:  98%|█████████▊| 13147/13389 [1:40:11<14:10,  3.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Attempt 1 failed for resume. Retrying...\n",
      "⚠ Attempt 2 failed for resume. Retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalizing resumes:  98%|█████████▊| 13148/13389 [1:40:35<39:11,  9.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Attempt 3 failed for resume. Retrying...\n",
      "❌ Failed to normalize resume after 3 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalizing resumes:  98%|█████████▊| 13172/13389 [1:42:23<15:06,  4.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Attempt 1 failed for resume. Retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalizing resumes:  99%|█████████▉| 13280/13389 [1:51:06<07:46,  4.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Attempt 1 failed for resume. Retrying...\n",
      "⚠ Attempt 2 failed for resume. Retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalizing resumes:  99%|█████████▉| 13281/13389 [1:51:30<18:25, 10.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Attempt 3 failed for resume. Retrying...\n",
      "❌ Failed to normalize resume after 3 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalizing resumes:  99%|█████████▉| 13313/13389 [1:54:10<06:10,  4.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Attempt 1 failed for resume. Retrying...\n",
      "⚠ Attempt 2 failed for resume. Retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalizing resumes:  99%|█████████▉| 13314/13389 [1:54:36<13:41, 10.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Attempt 3 failed for resume. Retrying...\n",
      "❌ Failed to normalize resume after 3 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalizing resumes: 100%|█████████▉| 13354/13389 [1:57:39<02:16,  3.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Attempt 1 failed for resume. Retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalizing resumes: 100%|█████████▉| 13360/13389 [1:58:21<02:54,  6.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Attempt 1 failed for resume. Retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalizing resumes: 100%|██████████| 13389/13389 [2:00:52<00:00,  1.85it/s]\n"
     ]
    }
   ],
   "source": [
    "# Then we loop through resumes, adding a progress bar \n",
    "for resume in tqdm(resumes, desc=\"Normalizing resumes\"):\n",
    "    resume_id = resume['resume_id']\n",
    "    # Skip resumes already processed\n",
    "    if resume_id in processed_ids:\n",
    "        continue\n",
    "    resume_text = resume['text']\n",
    "    # We normalize the text\n",
    "    norm_resume_text = normalize_resume(resume_text)# --> None if normalization failed after 3 attempts <--\n",
    "    # We join everything in a dictionnary for clarity\n",
    "    normalized_resume = {\n",
    "        'resume_id': resume_id,\n",
    "        'norm_text': norm_resume_text\n",
    "    }\n",
    "    # We put the result in a list\n",
    "    normalized_resumes.append(normalized_resume)\n",
    "    processed_ids.add(resume_id)\n",
    "    # We save a checkpoint (every 10 resumes)\n",
    "    if len(normalized_resumes) % 10 == 0:\n",
    "        with open(resume_checkpoint, \"w\") as f:\n",
    "            json.dump(normalized_resumes, f, indent=2)\n",
    "\n",
    "# Final save\n",
    "with open(resume_checkpoint, \"w\") as f:\n",
    "    json.dump(normalized_resumes, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edfbeba",
   "metadata": {},
   "source": [
    "The failed resume are still put into the JSON. I have to take care of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4689786d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of resumes normalized : 13314 / 13389\n",
      "Equivalent in %: 99.43983867353798 %\n"
     ]
    }
   ],
   "source": [
    "# Load normalized resumes\n",
    "with open(resume_checkpoint, \"r\") as f:\n",
    "    normalized_resumes = json.load(f)\n",
    "# We drop all the elements in normalized_resumes with 'norm_text'=None\n",
    "correctly_normalized_resumes = [r for r in normalized_resumes if r['norm_text'] is not None]\n",
    "# Let's see how much resumes where not correctly normalized\n",
    "print(\"Number of resumes normalized :\", len(correctly_normalized_resumes), \"/\", len(normalized_resumes))\n",
    "print(\"Equivalent in %:\", 100*len(correctly_normalized_resumes)/len(normalized_resumes), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90383fa",
   "metadata": {},
   "source": [
    " - Number of resumes normalized : 13314 / 13389\n",
    " - Equivalent in %: 99.43983867353798 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e643a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resave\n",
    "with open(resume_checkpoint, \"w\") as f:\n",
    "    json.dump(correctly_normalized_resumes, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f120465",
   "metadata": {},
   "source": [
    "#### Job descriptions (17014 elements) **DONE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "95691007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first prepare checkpoints (very long computation)\n",
    "## File to save intermediate results\n",
    "job_checkpoint = checkpoint_dir / \"normalized_jobs.json\"\n",
    "\n",
    "## Load existing checkpoint if available\n",
    "if job_checkpoint.exists():\n",
    "    print(\"Checkpoint found\")\n",
    "    with open(job_checkpoint, \"r\") as f:\n",
    "        normalized_jobs = json.load(f)\n",
    "    processed_ids = {r['job_id'] for r in normalized_jobs}\n",
    "    print(len(processed_ids), \"job postings\")\n",
    "else:\n",
    "    normalized_jobs = []\n",
    "    processed_ids = set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5dc139",
   "metadata": {},
   "source": [
    "After some tests, the whole extraction is expected to last about : 6h33min, mostly due to normalization through LLM calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6896813e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalizing job descriptions: 100%|██████████| 17014/17014 [3:41:08<00:00,  1.28it/s]  \n"
     ]
    }
   ],
   "source": [
    "for job in tqdm(jobs, desc=\"Normalizing job descriptions\"):\n",
    "    job_id = job['job_id']\n",
    "    # Skip resumes already processed\n",
    "    if job_id in processed_ids:\n",
    "        continue\n",
    "    job_text = job['text']\n",
    "    # We normalize the text\n",
    "    norm_job_des = normalize_job(job_text)# --> None if normalization failed after 3 attempts <--\n",
    "    if norm_job_des == None:\n",
    "        processed_ids.add(job_id)\n",
    "        continue\n",
    "    # We join everythin into a dictionnary for clarity\n",
    "    normalized_job = {\n",
    "        'job_id': job_id,\n",
    "        'job_description': norm_job_des\n",
    "    }\n",
    "    # We put the result in a list\n",
    "    normalized_jobs.append(normalized_job)\n",
    "    processed_ids.add(job_id)\n",
    "    # We save a checkpoint (every 100 job descriptions)\n",
    "    if len(normalized_jobs) % 100 == 0:\n",
    "        with open(job_checkpoint, \"w\") as f:\n",
    "            json.dump(normalized_jobs, f, indent=2)\n",
    "\n",
    "# Final save\n",
    "with open(job_checkpoint, \"w\") as f:\n",
    "    json.dump(normalized_jobs, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "22dddad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of resumes normalized : 17013 / 17014\n",
      "Equivalent in %: 99.99412248736334 %\n"
     ]
    }
   ],
   "source": [
    "# Load normalized jobs\n",
    "with open(job_checkpoint, \"r\") as f:\n",
    "    normalized_jobs = json.load(f)\n",
    "# Let's see how much resumes where not correctly normalized\n",
    "print(\"Number of resumes normalized :\", len(normalized_jobs), \"/\", len(jobs))\n",
    "print(\"Equivalent in %:\", 100*len(normalized_jobs)/len(jobs), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6faa34ea",
   "metadata": {},
   "source": [
    " - Number of job descriptions normalized : 17013 / 17014\n",
    " - Equivalent in %: 99.99 %"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293fb310",
   "metadata": {},
   "source": [
    "#### Fixing resume normalization **DONE**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226ddb5e",
   "metadata": {},
   "source": [
    "Some of the answers from the LLM are not formalized correctly.\n",
    "\n",
    "Experience is sometimes a list of dictionnaries (which breaks). \n",
    "- Ex : [{'Title': '...', 'Company': '...', 'Years': '...', ...}, {'Title': '...', 'Company': '...',...}]\n",
    "\n",
    "It has to be a list of strings.\n",
    "- Ex : ['Accountant – Omba Executive Leadership (2016-2018): ...', 'Accountant – Google (2015): ...']\n",
    "\n",
    "And we have the same problem with the education field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "a493d19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load normalized resumes\n",
    "with open(resume_checkpoint, \"r\") as f:\n",
    "    normalized_resumes = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2d7bc9",
   "metadata": {},
   "source": [
    "First, we need to verify all the resumes that were not normalized correctly, and why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9929dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5228 ERRORS\n"
     ]
    }
   ],
   "source": [
    "def validate_resume_record(norm_text:dict):\n",
    "    \"\"\"\n",
    "    Validates that:\n",
    "    - skills, experience, education, certifications and industries are a list of strings\n",
    "    \"\"\"\n",
    "    # Fields that must be lists of strings\n",
    "    list_fields = [\n",
    "        \"skills\",\n",
    "        \"experience\",\n",
    "        \"education\",\n",
    "        \"certifications\",\n",
    "        \"industries\"\n",
    "    ]\n",
    "\n",
    "    # verify they are lists\n",
    "    for field in list_fields:\n",
    "        if field in norm_text:\n",
    "            if not isinstance(norm_text[field], list):\n",
    "                return False\n",
    "            if not all(isinstance(ele, str) for ele in norm_text[field]):\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "# We search for any errors :\n",
    "count = 0\n",
    "for i in range(len(normalized_resumes)):\n",
    "    job_des = normalized_resumes[i]['norm_text']\n",
    "    if not validate_resume_record(job_des):\n",
    "        count += 1\n",
    "print(count, \"ERRORS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b31738",
   "metadata": {},
   "source": [
    "Now, we try detailing the errors for each field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "5fe49807",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_error(norm_text:dict, field:str):\n",
    "    \"\"\"\n",
    "    We will try seeing why some errors pop and \n",
    "    count the number of occurance for each type of errors\n",
    "    \"\"\"\n",
    "    record = norm_text.get(field)\n",
    "    error = \"\"\n",
    "\n",
    "    if not record:\n",
    "        error = \"no field\"\n",
    "    elif all(isinstance(item, str) for item in record):\n",
    "        error = \"no error\"\n",
    "    elif any(isinstance(item, dict) for item in record):\n",
    "        error = \"dictionnaries in list\"\n",
    "    elif any(isinstance(item, list) for item in record):\n",
    "        error = \"lists in list\"\n",
    "    else:\n",
    "        error = \"other\"\n",
    "    \n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2743e40",
   "metadata": {},
   "source": [
    "Let's see the errors in each field."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b84d91d",
   "metadata": {},
   "source": [
    "**experience**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "2a1e3fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --- SUMMARY --- \n",
      "Well normalized experience field (list with strings):\n",
      "8240 / 13314\n",
      "No experience field :\n",
      "15 / 13314\n",
      "Badly normalized experience field (list with dictionnaries):\n",
      "5048 / 13314\n",
      "Badly normalized experience field (list of lists):\n",
      "11 / 13314\n",
      "Badly normalized experience field (other):\n",
      "0 / 13314\n"
     ]
    }
   ],
   "source": [
    "experience_errors = {\n",
    "    \"no field\":0,\n",
    "    \"no error\":0,\n",
    "    \"dictionnaries in list\":0,\n",
    "    \"lists in list\":0,\n",
    "    \"other\":0\n",
    "}\n",
    "\n",
    "# We count the errors\n",
    "for i in range(len(normalized_resumes)):\n",
    "    norm_text = normalized_resumes[i].get('norm_text', {})\n",
    "    error = detailed_error(norm_text, 'experience')\n",
    "    experience_errors[error] += 1\n",
    "\n",
    "# Pretty print\n",
    "print(\" --- SUMMARY --- \")\n",
    "print(\"Well normalized experience field (list with strings):\")\n",
    "print(experience_errors['no error'], \"/\", len(normalized_resumes))\n",
    "print(\"No experience field :\")\n",
    "print(experience_errors['no field'], \"/\", len(normalized_resumes))\n",
    "print(\"Badly normalized experience field (list with dictionnaries):\")\n",
    "print(experience_errors['dictionnaries in list'], \"/\", len(normalized_resumes))\n",
    "print(\"Badly normalized experience field (list of lists):\")\n",
    "print(experience_errors['lists in list'], \"/\", len(normalized_resumes))\n",
    "print(\"Badly normalized experience field (other):\")\n",
    "print(experience_errors['other'], \"/\", len(normalized_resumes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c19acf7",
   "metadata": {},
   "source": [
    "Lists of errors that make the experience field badly normalized : \n",
    " - Well normalized experience field (list of strings) : 8240 / 13314\n",
    " - No experience field : 15 / 13314\n",
    " - Badly normalized experience field (list with dictionnaries) : 5048 / 13314\n",
    " - Badly normalized experience field (list of lists): 11 / 13314\n",
    " - Badly normalized experience field (other): 0 / 13314"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cff4386",
   "metadata": {},
   "source": [
    "**education**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "d053cb12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --- SUMMARY --- \n",
      "Well normalized education field (list with strings):\n",
      "7714 / 13314\n",
      "No education field :\n",
      "377 / 13314\n",
      "Badly normalized education field (list with dictionnaries):\n",
      "5212 / 13314\n",
      "Badly normalized education field (list of lists):\n",
      "11 / 13314\n",
      "Badly normalized education field (other):\n",
      "0 / 13314\n"
     ]
    }
   ],
   "source": [
    "education_errors = {\n",
    "    \"no error\":0,\n",
    "    \"no field\":0,\n",
    "    \"dictionnaries in list\":0,\n",
    "    \"lists in list\":0,\n",
    "    \"other\":0\n",
    "}\n",
    "\n",
    "# We count the errors\n",
    "for i in range(len(normalized_resumes)):\n",
    "    norm_text = normalized_resumes[i].get('norm_text', {})\n",
    "    error = detailed_error(norm_text, 'education')\n",
    "    education_errors[error] += 1\n",
    "\n",
    "# Pretty print\n",
    "print(\" --- SUMMARY --- \")\n",
    "print(\"Well normalized education field (list with strings):\")\n",
    "print(education_errors['no error'], \"/\", len(normalized_resumes))\n",
    "print(\"No education field :\")\n",
    "print(education_errors['no field'], \"/\", len(normalized_resumes))\n",
    "print(\"Badly normalized education field (list with dictionnaries):\")\n",
    "print(education_errors['dictionnaries in list'], \"/\", len(normalized_resumes))\n",
    "print(\"Badly normalized education field (list of lists):\")\n",
    "print(education_errors['lists in list'], \"/\", len(normalized_resumes))\n",
    "print(\"Badly normalized education field (other):\")\n",
    "print(education_errors['other'], \"/\", len(normalized_resumes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0d06cf",
   "metadata": {},
   "source": [
    "Lists of errors that make the education field badly normalized : \n",
    " - Well normalized education field (list of strings) : 7714 / 13314\n",
    " - No education field : 377 / 13314\n",
    " - Badly normalized education field (list with dictionnaries) : 5212 / 13314\n",
    " - Badly normalized education field (list of lists): 11 / 13314\n",
    " - Badly normalized education field (other): 0 / 13314"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fceab877",
   "metadata": {},
   "source": [
    "Then, we need to fix the errors in those fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "9c996d57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'agents.normalization_agent' from 'C:\\\\Users\\\\Sebastien\\\\Desktop\\\\LLM and GenAI\\\\Smart_Resume_to_Job_Matcher\\\\agents\\\\normalization_agent.py'>"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import agents.normalization_agent\n",
    "# Reload the file to take into account the changes\n",
    "importlib.reload(agents.normalization_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109af734",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.normalization_agent import coerce_to_strings_experience, coerce_to_strings_education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "ef70d73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_error(norm_text:dict, field:str):\n",
    "    \"\"\"\n",
    "    We will try seeing why some errors pop and \n",
    "    count the number of occurance for each type of errors\n",
    "    \"\"\"\n",
    "    record = norm_text.get(field)\n",
    "    error = \"\"\n",
    "\n",
    "    if not record:\n",
    "        error = \"no field\"\n",
    "    elif all(isinstance(item, str) for item in record):\n",
    "        error = \"no error\"\n",
    "    elif any(isinstance(item, dict) for item in record):\n",
    "        error = \"dictionnaries in list\"\n",
    "    elif any(isinstance(item, list) for item in record):\n",
    "        error = \"lists in list\"\n",
    "    else:\n",
    "        error = \"other\"\n",
    "    \n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903a1735",
   "metadata": {},
   "source": [
    "**experience**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "97561bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_error_experience(norm_text:dict):\n",
    "    \"\"\"\n",
    "    Now, we will use the functions defined in the normalization agent to fix \n",
    "    the issues made by the LLM.\n",
    "\n",
    "    Depending on the error, the fix is different, \n",
    "    so we will also use the previously define detailed_error function.\n",
    "    \"\"\"\n",
    "    field_value = norm_text.get(\"experience\")\n",
    "    error = detailed_error(norm_text, \"experience\")\n",
    "    if error == \"no field\":# We add an empty field\n",
    "        norm_text[\"experience\"] = []\n",
    "    elif error == \"dictionnaries in list\" or error == \"lists in list\":\n",
    "        norm_text[\"experience\"] = coerce_to_strings_experience(field_value)\n",
    "    return norm_text# Return the fixed dictionnary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "be7676e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(normalized_resumes)):\n",
    "    norm_text = normalized_resumes[i].get('norm_text', {})\n",
    "    normalized_resumes[i]['norm_text'] = fix_error_experience(norm_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad70472",
   "metadata": {},
   "source": [
    "**education**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "9473f506",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_error_education(norm_text:dict):\n",
    "    \"\"\"\n",
    "    Now, we will use the functions defined in the normalization agent to fix \n",
    "    the issues made by the LLM.\n",
    "\n",
    "    Depending on the error, the fix is different, \n",
    "    so we will also use the previously define detailed_error function.\n",
    "    \"\"\"\n",
    "    field_value = norm_text.get(\"education\")\n",
    "    error = detailed_error(norm_text, \"education\")\n",
    "    if error == \"no field\":# We add an empty field\n",
    "        norm_text[\"education\"] = []\n",
    "    elif error == \"dictionnaries in list\" or error == \"lists in list\":\n",
    "        norm_text[\"education\"] = coerce_to_strings_education(field_value)\n",
    "    return norm_text# Return the fixed dictionnary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "1079883e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(normalized_resumes)):\n",
    "    norm_text = normalized_resumes[i].get('norm_text', {})\n",
    "    normalized_resumes[i]['norm_text'] = fix_error_education(norm_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ce92e6",
   "metadata": {},
   "source": [
    "And we eventually save the resulting normalized files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "bb68111c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving\n",
    "with open(resume_checkpoint, \"w\") as f:\n",
    "    json.dump(normalized_resumes, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9de89fe",
   "metadata": {},
   "source": [
    "#### Fixing job normalization **DONE**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1772b3",
   "metadata": {},
   "source": [
    "We verify if there are any fields which types are not correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "7ed6bc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load normalized job descriptions\n",
    "with open(job_checkpoint, \"r\") as f:\n",
    "    normalized_jobs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "d6818026",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_job_record(job_description: dict) -> bool:\n",
    "    \"\"\"\n",
    "    Validates that:\n",
    "    - job_title, required_experience, required_education, industry are strings\n",
    "    - required_skills is a list of strings\n",
    "    \"\"\"\n",
    "\n",
    "    # Fields that must be strings\n",
    "    string_fields = [\n",
    "        \"job_title\",\n",
    "        \"required_experience\",\n",
    "        \"required_education\",\n",
    "        \"industry\"\n",
    "    ]\n",
    "\n",
    "    for field in string_fields:\n",
    "        if field in job_description and not isinstance(job_description[field], str):\n",
    "            return False\n",
    "\n",
    "    # Validate required_skills\n",
    "    if \"required_skills\" in job_description:\n",
    "        if not isinstance(job_description[\"required_skills\"], list):\n",
    "            return False\n",
    "        if not all(isinstance(skill, str) for skill in job_description[\"required_skills\"]):\n",
    "            return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "9cbbbbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# None don't work, job records are clean\n",
    "for i in range(len(normalized_jobs)):\n",
    "    job_des = normalized_jobs[i]['job_description']\n",
    "    if not validate_job_record(job_des):\n",
    "        print('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbf87ad",
   "metadata": {},
   "source": [
    "This code returns no 'ERROR' message, meaning that avery record was handled correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2c3b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving (not really necessary as nothing changed)\n",
    "with open(job_checkpoint, \"w\") as f:\n",
    "    json.dump(normalized_jobs, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e208a14d",
   "metadata": {},
   "source": [
    "# Generate embeddings **DONE**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d68e3e",
   "metadata": {},
   "source": [
    "### Complete embedding generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c955fc23",
   "metadata": {},
   "source": [
    "In this step, we will generate the embedding of both resumes and jobs thanks to a prompt. The objects will look like this :\n",
    "```python \n",
    "embedded_resumes = [\n",
    "    {\n",
    "    'resume_id': 1, \n",
    "    'resume_vector': List[float]\n",
    "    },\n",
    "    ...\n",
    "]\n",
    "\n",
    "embedded_jobs = [\n",
    "    {\n",
    "    'job_id': 1, \n",
    "    'job_vector': List[float]\n",
    "    },\n",
    "    ...\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf5fbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import embeddings.embedding_engine\n",
    "# Reload the file to take into account the changes\n",
    "importlib.reload(embeddings.embedding_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "136c78b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from embeddings.embedding_engine import embed_resume_nomic, embed_job_nomic, embed_resume_BGE, embed_job_BGE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b5bedb",
   "metadata": {},
   "source": [
    "#### Resumes (13314 elements) **DONE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6f5952c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "03ae09cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkpoint_verification_resumes(normalized_resumes_file:Path, embedded_path:Path):\n",
    "    # Load normalized resumes\n",
    "    with open(normalized_resumes_file, \"r\") as f:\n",
    "        normalized_resumes = json.load(f)\n",
    "        print(\"Normalized files imported\")\n",
    "\n",
    "    # Load existing embeddings if checkpoint exists (restart-safe)\n",
    "    if embedded_path.exists():\n",
    "        print(\"Checkpoint found\")\n",
    "        with open(embedded_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            embedded = json.load(f)\n",
    "        processed_ids = {r['resume_id'] for r in embedded}\n",
    "        print(len(processed_ids), \"resume embeddings found\")\n",
    "    else:\n",
    "        print(\"No checkpoint found\")\n",
    "        embedded = []\n",
    "        processed_ids = set()\n",
    "    \n",
    "    return normalized_resumes, embedded, processed_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ea940b",
   "metadata": {},
   "source": [
    "**NOMIC embedding model** (~20 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c30691a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized files imported\n",
      "Checkpoint found\n",
      "13296 resume embeddings found\n"
     ]
    }
   ],
   "source": [
    "## First we intitialize our storage (we download it if some was already computed)\n",
    "# Paths\n",
    "normalized_resumes_file = Path(\"checkpoints/normalized_resumes.json\")\n",
    "resume_emb_file = Path(\"checkpoints/resume_embeddings_nomic.json\")\n",
    "normalized_resumes, embedded_resumes, processed_ids = checkpoint_verification_resumes(normalized_resumes_file, resume_emb_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c850c1c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating resume embeddings...:  23%|██▎       | 3113/13314 [01:45<15:36, 10.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Failed embedding for resume 3114: unhashable type: 'dict'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating resume embeddings...:  23%|██▎       | 3128/13314 [01:45<07:46, 21.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Failed embedding for resume 3125: unhashable type: 'dict'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating resume embeddings...:  27%|██▋       | 3555/13314 [02:05<03:51, 42.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Failed embedding for resume 3551: unhashable type: 'dict'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating resume embeddings...:  29%|██▉       | 3828/13314 [02:19<07:18, 21.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Failed embedding for resume 3826: unhashable type: 'dict'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating resume embeddings...:  30%|██▉       | 3986/13314 [02:25<03:10, 49.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Failed embedding for resume 3985: unhashable type: 'dict'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating resume embeddings...:  33%|███▎      | 4344/13314 [02:46<04:48, 31.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Failed embedding for resume 4341: unhashable type: 'dict'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating resume embeddings...:  48%|████▊     | 6431/13314 [04:55<08:32, 13.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Failed embedding for resume 6438: unhashable type: 'dict'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating resume embeddings...:  56%|█████▌    | 7407/13314 [06:02<01:54, 51.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Failed embedding for resume 7417: unhashable type: 'dict'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating resume embeddings...:  62%|██████▏   | 8216/13314 [07:11<19:15,  4.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Failed embedding for resume 8235: unhashable type: 'dict'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating resume embeddings...:  68%|██████▊   | 9097/13314 [08:21<01:25, 49.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Failed embedding for resume 9127: unhashable type: 'dict'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating resume embeddings...:  69%|██████▉   | 9200/13314 [08:30<01:21, 50.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Failed embedding for resume 9225: unhashable type: 'dict'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating resume embeddings...:  70%|██████▉   | 9276/13314 [08:38<01:32, 43.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Failed embedding for resume 9304: unhashable type: 'dict'\n",
      "⚠ Failed embedding for resume 9309: unhashable type: 'dict'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating resume embeddings...:  76%|███████▌  | 10142/13314 [10:00<03:47, 13.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Failed embedding for resume 10177: unhashable type: 'dict'\n",
      "⚠ Failed embedding for resume 10178: unhashable type: 'dict'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating resume embeddings...:  81%|████████▏ | 10822/13314 [11:08<12:14,  3.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Failed embedding for resume 10874: unhashable type: 'dict'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating resume embeddings...:  95%|█████████▍| 12596/13314 [14:12<00:15, 47.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Failed embedding for resume 12653: unhashable type: 'dict'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating resume embeddings...:  95%|█████████▍| 12615/13314 [14:12<00:13, 53.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Failed embedding for resume 12670: unhashable type: 'dict'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating resume embeddings...: 100%|██████████| 13314/13314 [15:32<00:00, 14.28it/s]\n"
     ]
    }
   ],
   "source": [
    "##  Then generate embeddings for remaining resumes\n",
    "for r in tqdm(normalized_resumes, desc=\"Generating resume embeddings...\"):\n",
    "    resume_id = r[\"resume_id\"]\n",
    "    # Skip resumes already processed\n",
    "    if resume_id in processed_ids:\n",
    "        continue\n",
    "    try:\n",
    "        resume_vector = embed_resume_nomic(r[\"norm_text\"])\n",
    "        embedded_resume = {\n",
    "            'resume_id': resume_id,\n",
    "            'resume_vector': resume_vector\n",
    "        }\n",
    "        embedded_resumes.append(embedded_resume)\n",
    "        processed_ids.add(resume_id)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Failed embedding for resume {resume_id}: {e}\")\n",
    "    \n",
    "    # We save a checkpoint (every 100 embedded resumes)\n",
    "    if len(embedded_resumes) % 100 == 0:\n",
    "        with open(resume_emb_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(embedded_resumes, f)\n",
    "\n",
    "# Save embeddings to disk for later use\n",
    "with open(resume_emb_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(embedded_resumes, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2196f918",
   "metadata": {},
   "source": [
    "Despite thiniking we took care of the typing issue in the previous phase, apparently, normalization was not strict enough and some files are still rejected from embedding because of typing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "90fe1d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings: 13296 resumes\n",
      "Number of resumes not embedded : 18\n",
      "In % : 0.135196034249662\n"
     ]
    }
   ],
   "source": [
    "print(f\"Generated embeddings: {len(embedded_resumes)} resumes\")\n",
    "\n",
    "print(f\"Number of resumes not embedded : {len(normalized_resumes)-len(embedded_resumes)}\")\n",
    "print(f\"In % : {100*(len(normalized_resumes)-len(embedded_resumes))/len(normalized_resumes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430ca316",
   "metadata": {},
   "source": [
    "However, only 18 files were not correctly embedded, which represents less than 0.2% of all resumes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13183ad",
   "metadata": {},
   "source": [
    "**BGE embedding model** (~30 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9e819a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized files imported\n",
      "Checkpoint found\n",
      "13296 resume embeddings found\n"
     ]
    }
   ],
   "source": [
    "normalized_resumes_file = Path(\"checkpoints/normalized_resumes.json\")\n",
    "resume_emb_file = Path(\"checkpoints/resume_embeddings_bge.json\")\n",
    "normalized_resumes, embedded_resumes, processed_ids = checkpoint_verification_resumes(normalized_resumes_file, resume_emb_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9788e4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating resume embeddings...:  23%|██▎       | 3112/13314 [06:31<17:53,  9.51it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Failed embedding for resume 3114: unhashable type: 'dict'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating resume embeddings...:  23%|██▎       | 3123/13314 [06:32<12:27, 13.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Failed embedding for resume 3125: unhashable type: 'dict'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating resume embeddings...:  27%|██▋       | 3548/13314 [07:14<10:51, 14.98it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Failed embedding for resume 3551: unhashable type: 'dict'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating resume embeddings...:  29%|██▊       | 3823/13314 [07:45<14:43, 10.75it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Failed embedding for resume 3826: unhashable type: 'dict'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating resume embeddings...:  30%|██▉       | 3982/13314 [08:00<12:04, 12.87it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Failed embedding for resume 3985: unhashable type: 'dict'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating resume embeddings...:  33%|███▎      | 4338/13314 [08:42<10:30, 14.24it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Failed embedding for resume 4341: unhashable type: 'dict'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating resume embeddings...:  48%|████▊     | 6426/13314 [13:00<12:42,  9.03it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Failed embedding for resume 6438: unhashable type: 'dict'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating resume embeddings...:  56%|█████▌    | 7406/13314 [15:12<06:35, 14.93it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Failed embedding for resume 7417: unhashable type: 'dict'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating resume embeddings...:  62%|██████▏   | 8215/13314 [17:20<32:14,  2.64it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Failed embedding for resume 8235: unhashable type: 'dict'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating resume embeddings...:  68%|██████▊   | 9093/13314 [19:33<05:48, 12.11it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Failed embedding for resume 9127: unhashable type: 'dict'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating resume embeddings...:  69%|██████▉   | 9192/13314 [19:48<05:22, 12.78it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Failed embedding for resume 9225: unhashable type: 'dict'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating resume embeddings...:  70%|██████▉   | 9270/13314 [20:02<05:07, 13.16it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Failed embedding for resume 9304: unhashable type: 'dict'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating resume embeddings...:  70%|██████▉   | 9275/13314 [20:02<04:48, 14.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Failed embedding for resume 9309: unhashable type: 'dict'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating resume embeddings...:  76%|███████▌  | 10135/13314 [22:22<05:28,  9.66it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Failed embedding for resume 10177: unhashable type: 'dict'\n",
      "⚠ Failed embedding for resume 10178: unhashable type: 'dict'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating resume embeddings...:  81%|████████▏ | 10822/13314 [24:14<17:28,  2.38it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Failed embedding for resume 10874: unhashable type: 'dict'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating resume embeddings...:  95%|█████████▍| 12591/13314 [29:11<00:54, 13.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Failed embedding for resume 12653: unhashable type: 'dict'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating resume embeddings...:  95%|█████████▍| 12606/13314 [29:12<00:51, 13.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Failed embedding for resume 12670: unhashable type: 'dict'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating resume embeddings...: 100%|██████████| 13314/13314 [31:18<00:00,  7.09it/s]\n"
     ]
    }
   ],
   "source": [
    "##  Then generate embeddings for remaining resumes\n",
    "for r in tqdm(normalized_resumes, desc=\"Generating resume embeddings...\"):\n",
    "    resume_id = r[\"resume_id\"]\n",
    "    # Skip resumes already processed\n",
    "    if resume_id in processed_ids:\n",
    "        continue\n",
    "    try:\n",
    "        resume_vector = embed_resume_BGE(r[\"norm_text\"])\n",
    "        embedded_resume = {\n",
    "            'resume_id': resume_id,\n",
    "            'resume_vector': resume_vector\n",
    "        }\n",
    "        embedded_resumes.append(embedded_resume)\n",
    "        processed_ids.add(resume_id)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Failed embedding for resume {resume_id}: {e}\")\n",
    "    \n",
    "    # We save a checkpoint (every 100 embedded resumes)\n",
    "    if len(embedded_resumes) % 100 == 0:\n",
    "        with open(resume_emb_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(embedded_resumes, f)\n",
    "\n",
    "# Save embeddings to disk for later use\n",
    "with open(resume_emb_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(embedded_resumes, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6c86d2d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings: 13296 resumes\n",
      "Number of resumes not embedded : 18\n",
      "In % : 0.135196034249662\n"
     ]
    }
   ],
   "source": [
    "print(f\"Generated embeddings: {len(embedded_resumes)} resumes\")\n",
    "\n",
    "print(f\"Number of resumes not embedded : {len(normalized_resumes)-len(embedded_resumes)}\")\n",
    "print(f\"In % : {100*(len(normalized_resumes)-len(embedded_resumes))/len(normalized_resumes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3586768",
   "metadata": {},
   "source": [
    "Here too, only 18 files were not correctly embedded, which represents less than 0.2% of all resumes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c27e80",
   "metadata": {},
   "source": [
    "#### Job descriptions (17013 elements) **DONE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1d544b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkpoint_verification_jobs(normalized_jobs_file:Path, embedded_path:Path):\n",
    "    # Load normalized resumes\n",
    "    with open(normalized_jobs_file, \"r\") as f:\n",
    "        normalized_jobs = json.load(f)\n",
    "        print(\"Normalized files imported\")\n",
    "\n",
    "    # Load existing embeddings if checkpoint exists (restart-safe)\n",
    "    if embedded_path.exists():\n",
    "        print(\"Checkpoint found\")\n",
    "        with open(embedded_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            embedded = json.load(f)\n",
    "        processed_ids = {r['job_id'] for r in embedded}\n",
    "        print(len(processed_ids), \"job embeddings found\")\n",
    "    else:\n",
    "        print(\"No checkpoint found\")\n",
    "        embedded = []\n",
    "        processed_ids = set()\n",
    "    \n",
    "    return normalized_jobs, embedded, processed_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7cc7e316",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_job(normalized_jobs, embedded_jobs, processed_ids, job_emb_file, embedding_function):\n",
    "    for j in tqdm(normalized_jobs, desc=\"Generating job embeddings...\"):\n",
    "        job_id = j[\"job_id\"]\n",
    "        # Skip resumes already processed\n",
    "        if job_id in processed_ids:\n",
    "            continue\n",
    "        try:\n",
    "            job_vector = embedding_function(j[\"job_description\"])\n",
    "            embedded_job = {\n",
    "                'job_id': job_id,\n",
    "                'job_vector': job_vector\n",
    "            }\n",
    "            embedded_jobs.append(embedded_job)\n",
    "            processed_ids.add(job_id)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ Failed embedding for job {job_id}: {e}\")\n",
    "        \n",
    "        # We save a checkpoint (every 100 embedded jobs)\n",
    "        if len(embedded_jobs) % 100 == 0:\n",
    "            with open(job_emb_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(embedded_jobs, f)\n",
    "\n",
    "    # Save embeddings to disk for later use\n",
    "    with open(job_emb_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(embedded_jobs, f)\n",
    "    \n",
    "    return embedded_jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ba7d3d",
   "metadata": {},
   "source": [
    "**NOMIC embedding model** (~23 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5446e334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized files imported\n",
      "Checkpoint found\n",
      "17013 job embeddings found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating job embeddings...: 100%|██████████| 17013/17013 [00:00<00:00, 1652869.78it/s]\n"
     ]
    }
   ],
   "source": [
    "## First we intitialize our storage (we download it if some was already computed)\n",
    "# Paths to checkpointed normalized data\n",
    "normalized_jobs_file = Path(\"checkpoints/normalized_jobs.json\")\n",
    "job_emb_file = Path(\"checkpoints/jobs_embeddings_nomic.json\")\n",
    "# Checkpoint verification\n",
    "normalized_jobs, embedded_jobs, processed_ids = checkpoint_verification_jobs(normalized_jobs_file, job_emb_file)\n",
    "# Embedding generation\n",
    "embedded_jobs = embed_job(normalized_jobs, embedded_jobs, processed_ids, job_emb_file, embed_job_nomic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "af520fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings: 17013 job descriptions\n",
      "Number of resumes not embedded : 0\n",
      "In % : 0.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Generated embeddings: {len(embedded_jobs)} job descriptions\")\n",
    "\n",
    "print(f\"Number of resumes not embedded : {len(normalized_jobs)-len(embedded_jobs)}\")\n",
    "print(f\"In % : {100*(len(normalized_jobs)-len(embedded_jobs))/len(normalized_jobs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4164048",
   "metadata": {},
   "source": [
    "**BGE embedding model** (~30 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "48b055c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized files imported\n",
      "Checkpoint found\n",
      "17013 job embeddings found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating job embeddings...: 100%|██████████| 17013/17013 [00:00<00:00, 1231494.10it/s]\n"
     ]
    }
   ],
   "source": [
    "## First we intitialize our storage (we download it if some was already computed)\n",
    "# Paths to checkpointed normalized data\n",
    "normalized_jobs_file = Path(\"checkpoints/normalized_jobs.json\")\n",
    "job_emb_file = Path(\"checkpoints/jobs_embeddings_bge.json\")\n",
    "# Checkpoint verification\n",
    "normalized_jobs, embedded_jobs, processed_ids = checkpoint_verification_jobs(normalized_jobs_file, job_emb_file)\n",
    "# Embedding generation\n",
    "embedded_jobs = embed_job(normalized_jobs, embedded_jobs, processed_ids, job_emb_file, embed_job_BGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "837d80a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings: 17013 job descriptions\n",
      "Number of resumes not embedded : 0\n",
      "In % : 0.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Generated embeddings: {len(embedded_jobs)} job descriptions\")\n",
    "\n",
    "print(f\"Number of resumes not embedded : {len(normalized_jobs)-len(embedded_jobs)}\")\n",
    "print(f\"In % : {100*(len(normalized_jobs)-len(embedded_jobs))/len(normalized_jobs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcb818e",
   "metadata": {},
   "source": [
    "However, when it comes to jobs, just like normalization, no job description impeded the process. This is probably because the typing was simpler, no dictionnary inside dictionnaries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3126bb6b",
   "metadata": {},
   "source": [
    "# Cosine similarity and ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5ddcfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the parent folder of ingestion to sys.path\n",
    "project_root = Path(\"..\").resolve()  # notebooks/ is one level down\n",
    "sys.path.append(str(project_root))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5d7bea",
   "metadata": {},
   "source": [
    "## Format conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4c4037",
   "metadata": {},
   "source": [
    "For now, I have embeddings looking like this : \n",
    "```python \n",
    "embedded_resumes = [\n",
    "    {\n",
    "    'resume_id': 1, \n",
    "    'resume_vector': List[float]\n",
    "    },\n",
    "    ...\n",
    "]\n",
    "\n",
    "embedded_jobs = [\n",
    "    {\n",
    "    'job_id': 1, \n",
    "    'job_vector': List[float]\n",
    "    },\n",
    "    ...\n",
    "]\n",
    "```\n",
    "However, this format takes more space and is harder to manipulate. So, we will first convert our lists into dictionnaries, whose keys will be the embeddings' id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d5817ab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'embeddings.embedding_format_conversion' from 'C:\\\\Users\\\\Sebastien\\\\Desktop\\\\LLM and GenAI\\\\Smart_Resume_to_Job_Matcher\\\\embeddings\\\\embedding_format_conversion.py'>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import embeddings.embedding_format_conversion\n",
    "# Reload the file to take into account the changes\n",
    "importlib.reload(embeddings.embedding_format_conversion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04f37201",
   "metadata": {},
   "outputs": [],
   "source": [
    "from embeddings.embedding_format_conversion import lists_to_id_vector_dicts\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ffb99178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded resumes (Nomic) imported\n",
      "Embedded resumes (BGE) imported\n",
      "Embedded job descriptions (Nomic) imported\n",
      "Embedded job descriptions (BGE) imported\n"
     ]
    }
   ],
   "source": [
    "## First we import the embeddings\n",
    "# Paths\n",
    "resume_emb_file_nomic = Path(\"checkpoints/resume_embeddings_nomic.json\")\n",
    "resume_emb_file_bge = Path(\"checkpoints/resume_embeddings_bge.json\")\n",
    "job_emb_file_nomic = Path(\"checkpoints/jobs_embeddings_nomic.json\")\n",
    "job_emb_file_bge = Path(\"checkpoints/jobs_embeddings_bge.json\")\n",
    "\n",
    "# Load embedded resumes\n",
    "with open(resume_emb_file_nomic, \"r\") as f:\n",
    "    embedded_resumes_nomic = json.load(f)\n",
    "    print(\"Embedded resumes (Nomic) imported\")\n",
    "with open(resume_emb_file_bge, \"r\") as f:\n",
    "    embedded_resumes_bge = json.load(f)\n",
    "    print(\"Embedded resumes (BGE) imported\")\n",
    "\n",
    "# Load embedded job descriptions\n",
    "with open(job_emb_file_nomic, \"r\") as f:\n",
    "    embedded_jobs_nomic = json.load(f)\n",
    "    print(\"Embedded job descriptions (Nomic) imported\")\n",
    "with open(job_emb_file_bge, \"r\") as f:\n",
    "    embedded_jobs_bge = json.load(f)\n",
    "    print(\"Embedded job descriptions (BGE) imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "94e50894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resume IDs (Nomic): [1, 2, 3, 4, 5]\n",
      "Job IDs (Nomic): [1, 2, 3, 4, 5]\n",
      "Resume IDs (BGE): [1, 2, 3, 4, 5]\n",
      "Job IDs (BGE): [1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "# Then, convert them into smaller dictionnaries for better storage\n",
    "embedded_resumes_nomic_dict, embedded_jobs_nomic_dict = lists_to_id_vector_dicts(embedded_resumes_nomic, embedded_jobs_nomic)\n",
    "embedded_resumes_bge_dict, embedded_jobs_bge_dict = lists_to_id_vector_dicts(embedded_resumes_bge, embedded_jobs_bge)\n",
    "\n",
    "print(\"Resume IDs (Nomic):\", list(embedded_resumes_nomic_dict.keys())[:5])\n",
    "print(\"Job IDs (Nomic):\", list(embedded_jobs_nomic_dict.keys())[:5])\n",
    "print(\"Resume IDs (BGE):\", list(embedded_resumes_bge_dict.keys())[:5])\n",
    "print(\"Job IDs (BGE):\", list(embedded_jobs_bge_dict.keys())[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef85106",
   "metadata": {},
   "source": [
    "## Scoring and matching (Mettre l'ensemble des embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6f8f72",
   "metadata": {},
   "source": [
    "Now that the embeddings are converted back to dictionnaries, we can use them for scoring and ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ed2adcdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'match_engine_and_explanation.match_engine' from 'C:\\\\Users\\\\Sebastien\\\\Desktop\\\\LLM and GenAI\\\\Smart_Resume_to_Job_Matcher\\\\match_engine_and_explanation\\\\match_engine.py'>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import match_engine_and_explanation.match_engine\n",
    "# Reload the file to take into account the changes\n",
    "importlib.reload(match_engine_and_explanation.match_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6b08b47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ingestion.preprocess\n",
    "importlib.reload(ingestion.preprocess)\n",
    "\n",
    "from ingestion.preprocess import resumes_to_raw_text, jobs_to_raw_text\n",
    "\n",
    "resumes = resumes_to_raw_text(\"../data/resumes/resumes.csv\")\n",
    "jobs = jobs_to_raw_text(\"../data/jobs/job_postings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465b0db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEMP - the different embeddings for resumes and job descriptions - TEMP\n",
    "embedded_resumes_nomic_dict, embedded_jobs_nomic_dict\n",
    "embedded_resumes_bge_dict, embedded_jobs_bge_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5414f7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from match_engine_and_explanation.match_engine import match_jobs_to_resume, match_resumes_to_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6538387a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedded_resumes = {resume_id:resume_vector, ...}\n",
    "# embedded_jobs = {job_id:job_vector, ...}\n",
    "\n",
    "job_id = 78\n",
    "embedding = embedded_jobs_bge_dict[job_id]\n",
    "\n",
    "results = match_resumes_to_job(\n",
    "    job_id=job_id,\n",
    "    job_embedding=embedding,\n",
    "    resume_embeddings=embedded_resumes_bge_dict,\n",
    "    top_k=5\n",
    ")\n",
    "\n",
    "# Displaying the result\n",
    "print(f\"Job ID: {job_id}\")\n",
    "print(\"=\" * 30, \"Job description\", \"=\" * 30)\n",
    "print(jobs[job_id-1]['text'])\n",
    "\n",
    "for r in results:\n",
    "    print(\"=\" * 80)\n",
    "    resume_id = r['resume_id']\n",
    "    print(f\"Resume ID: {resume_id}\")\n",
    "    print(f\"Resume's text : \")\n",
    "    print(resumes[resume_id-1]['text'])\n",
    "    print(f\"Similarity score: {r['score']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cf7548a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from match_engine_and_explanation.match_engine import score_jobs_for_resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f35379d",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_id = 456\n",
    "embedding = embedded_resumes_bge_dict[resume_id]\n",
    "\n",
    "results = match_jobs_to_resume(\n",
    "    resume_id=resume_id,\n",
    "    resume_embedding=embedding,\n",
    "    job_embeddings=embedded_jobs_bge_dict,\n",
    "    top_k=5\n",
    ")\n",
    "\n",
    "\n",
    "# Displaying the result\n",
    "print(f\"Resume ID: {resume_id}\")\n",
    "print(\"=\" * 30, \"Resume's text\", \"=\" * 30)\n",
    "print(resumes[resume_id-1]['text'])\n",
    "\n",
    "for r in results:\n",
    "    print(\"=\" * 80)\n",
    "    job_id = r['job_id']\n",
    "    print(f\"Job ID: {job_id}\")\n",
    "    print(f\"Job description : \")\n",
    "    print(jobs[job_id-1]['text'])\n",
    "    print(f\"Similarity score: {r['score']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9549a4",
   "metadata": {},
   "source": [
    "# LLM-based explanation (A FAIRE AVEC LES NOUVEAUX EMBEDDINGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab02821",
   "metadata": {},
   "outputs": [],
   "source": [
    "import match_engine_and_explanation.llm_explanation\n",
    "# Reload the file to take into account the changes\n",
    "importlib.reload(match_engine_and_explanation.llm_explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0154056c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from match_engine_and_explanation.llm_explanation import generate_match_explanation\n",
    "\n",
    "# normalized_jobs = [{id:normalized_job_i, ...}]\n",
    "# normalized_resumes = [{id:normalized_resume_i, ...}]\n",
    "# results = [{\"job_id\": job_id[str], \"resume_id\": resume_id, \"score\": score}]\n",
    "r_0 = results[0]\n",
    "job_id = r_0['job_id']\n",
    "resume_id = r_0['resume_id']\n",
    "score = r_0['score']\n",
    "\n",
    "job = normalized_job# \"job_description\" where \"job_id\" = job_id\n",
    "resume = normalized_resume# \"norm_text\" where \"resume_id\" = resume_id\n",
    "\n",
    "generate_match_explanation(normalized_job, normalized_resume, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ebfb1c",
   "metadata": {},
   "source": [
    "# ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d66fb355",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26e31fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e2d894",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "351cafa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit\n",
    "import fastapi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Smart Resume Matcher",
   "language": "python",
   "name": "smart-resume-matcher"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
